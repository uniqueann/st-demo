import streamlit as st
import siliconflow as sf
from dotenv import load_dotenv
import os

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
sf.api_key = os.getenv('SILICONFLOW_API_KEY')

# è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹
def get_available_models():
    try:
        # è·å–æ‰€æœ‰æ¨¡å‹åˆ—è¡¨
        models = sf.Model.list()
        
        # æŒ‰ç±»å‹åˆ†ç±»æ¨¡å‹
        categorized_models = {
            "LLM": [],
            "VLM": [],
            "Image": [],
            "Video": [],
            "Audio": []
        }
        
        for model in models:
            # æ ¹æ®æ¨¡å‹çš„capabilitiesæˆ–å…¶ä»–å±æ€§æ¥åˆ†ç±»
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„APIè¿”å›ç»“æ„æ¥è°ƒæ•´
            if model.capabilities.get("text"):
                if model.capabilities.get("vision"):
                    categorized_models["VLM"].append(model)
                else:
                    categorized_models["LLM"].append(model)
            if model.capabilities.get("image"):
                categorized_models["Image"].append(model)
            if model.capabilities.get("video"):
                categorized_models["Video"].append(model)
            if model.capabilities.get("audio"):
                categorized_models["Audio"].append(model)
        
        return categorized_models
    except Exception as e:
        st.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥ï¼š{str(e)}")
        return None

def create_text_generation_tab(models):
    st.header("æ–‡æœ¬ç”Ÿæˆ")
    
    with st.container():
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # é€‰æ‹©æ¨¡å‹ç±»å‹
            model_type = st.radio("é€‰æ‹©æ¨¡å‹ç±»å‹", ["LLM", "VLM"])
            
            # è·å–é€‰å®šç±»å‹çš„å¯ç”¨æ¨¡å‹
            available_models = models.get(model_type, [])
            if not available_models:
                st.warning(f"å½“å‰æ²¡æœ‰å¯ç”¨çš„{model_type}æ¨¡å‹")
                return
            
            # é€‰æ‹©å…·ä½“æ¨¡å‹
            selected_model = st.selectbox(
                "é€‰æ‹©æ¨¡å‹",
                options=available_models,
                format_func=lambda x: x.name
            )
            
            # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
            with st.expander("æ¨¡å‹ä¿¡æ¯", expanded=False):
                st.json(selected_model.to_dict())
            
            # æ¨¡å‹å‚æ•°è®¾ç½®
            with st.expander("æ¨¡å‹å‚æ•°è®¾ç½®", expanded=True):
                temperature = st.slider(
                    "Temperature",
                    min_value=0.0,
                    max_value=1.0,
                    value=0.7,
                    step=0.1
                )
                max_tokens = st.number_input(
                    "æœ€å¤§ä»¤ç‰Œæ•°",
                    min_value=1,
                    max_value=selected_model.max_tokens if hasattr(selected_model, 'max_tokens') else 4096,
                    value=1000
                )
            
            # VLMæ¨¡å‹ç‰¹æœ‰çš„å›¾ç‰‡ä¸Šä¼ åŠŸèƒ½
            if model_type == "VLM":
                uploaded_file = st.file_uploader("ä¸Šä¼ å›¾ç‰‡", type=['png', 'jpg', 'jpeg'])
                if uploaded_file is not None:
                    st.image(uploaded_file, caption="ä¸Šä¼ çš„å›¾ç‰‡")
            
            prompt = st.text_area("è¾“å…¥æç¤ºè¯", height=200)
            
            if st.button("ç”Ÿæˆ", use_container_width=True):
                with st.spinner("ç”Ÿæˆä¸­..."):
                    try:
                        # æ„å»ºè¯·æ±‚å‚æ•°
                        params = {
                            "temperature": temperature,
                            "max_tokens": max_tokens,
                            "prompt": prompt
                        }
                        
                        # å¦‚æœæ˜¯VLMä¸”æœ‰ä¸Šä¼ å›¾ç‰‡ï¼Œæ·»åŠ å›¾ç‰‡å‚æ•°
                        if model_type == "VLM" and uploaded_file:
                            params["image"] = uploaded_file
                        
                        # è°ƒç”¨APIç”Ÿæˆå†…å®¹
                        response = selected_model.generate(**params)
                        
                        st.success("ç”Ÿæˆå®Œæˆï¼")
                        st.markdown(response.text)
                        
                    except Exception as e:
                        st.error(f"ç”Ÿæˆå¤±è´¥ï¼š{str(e)}")
        
        with col2:
            st.subheader("ç”Ÿæˆå†å²")
            st.info("è¿™é‡Œæ˜¾ç¤ºå†å²ç”Ÿæˆè®°å½•")

def main():
    st.title("ğŸ¯ SiliconFlow AI Studio")
    
    # è·å–å¯ç”¨æ¨¡å‹
    models = get_available_models()
    if not models:
        st.error("æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®")
        return
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    tabs = st.tabs(['æ–‡æœ¬ç”Ÿæˆ', 'å›¾åƒç”Ÿæˆ', 'è§†é¢‘ç”Ÿæˆ', 'è¯­éŸ³ç”Ÿæˆ'])
    
    with tabs[0]:
        create_text_generation_tab(models)
    with tabs[1]:
        create_image_generation_tab(models)
    with tabs[2]:
        create_video_generation_tab(models)
    with tabs[3]:
        create_audio_generation_tab(models)

if __name__ == "__main__":
    main()
